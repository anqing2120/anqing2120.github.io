<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="/images/icons2/favicon-16x16.png?v=2.8.0" type="image/png" sizes="16x16"><link rel="icon" href="/images/icons2/favicon-32x32.png?v=2.8.0" type="image/png" sizes="32x32"><link rel="apple-touch-icon" href="/images/icons2/apple-touch-icon.png?v=2.8.0" sizes="180x180"><link rel="mask-icon" href="/images/icons2/stun-logo.svg?v=2.8.0" color="#54bcff"><meta name="msapplication-TileImage" content="/images/icons2/favicon-144x144.png"><meta name="msapplication-TileColor" content="#000000"><meta name="description" content="Approaching (Almost) Any Machine Learning Problem        解决几乎所有机器学习问题 很有气势！                      1 环境       安装环境 要指定Python版本 pip的版本 【很重要】 安装软件包 使用pip 【conda库不全】 1conda create -n en">
<meta property="og:type" content="article">
<meta property="og:title" content="ML_learning">
<meta property="og:url" content="https://anqing2120.github.io/2023/12/26/ML-learning/index.html">
<meta property="og:site_name" content="择善而固执之">
<meta property="og:description" content="Approaching (Almost) Any Machine Learning Problem        解决几乎所有机器学习问题 很有气势！                      1 环境       安装环境 要指定Python版本 pip的版本 【很重要】 安装软件包 使用pip 【conda库不全】 1conda create -n en">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://anqing2120.github.io/2023/12/26/ML-learning/ML-learning.assets/%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97t-SNE%E8%81%9A%E7%B1%BB.png">
<meta property="og:image" content="https://anqing2120.github.io/2023/12/26/ML-learning/ML-learning.assets/%E8%BF%87%E6%8B%9F%E5%90%88.png">
<meta property="og:image" content="https://anqing2120.github.io/ML-learning.assets/%E8%BF%87%E6%8B%9F%E5%90%88-1700019734922-3.png">
<meta property="article:published_time" content="2023-12-26T12:51:54.000Z">
<meta property="article:modified_time" content="2023-12-26T12:53:24.685Z">
<meta property="article:author" content="一诚">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://anqing2120.github.io/2023/12/26/ML-learning/ML-learning.assets/%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97t-SNE%E8%81%9A%E7%B1%BB.png"><title>ML_learning | 择善而固执之</title><link ref="canonical" href="https://anqing2120.github.io/2023/12/26/ML-learning/"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.8.0"><link rel="stylesheet" href="css/custom.css"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  assistSearch: undefined,
  fontIcon: {"prompt":{"success":"fas fa-check-circle","info":"fas fa-arrow-circle-right","warning":"fas fa-exclamation-circle","error":"fas fa-times-circle"},"copyBtn":"fas fa-copy"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":false,"scrollDownIcon":false},
  postWidget: {"endText":true},
  nightMode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"simple","highlight":"ocean","wordWrap":false},
  reward: false,
  fancybox: false,
  zoomImage: {"gapAside":"20px"},
  galleryWaterfall: undefined,
  lazyload: false,
  pjax: undefined,
  externalLink: {"icon":{"enable":true,"name":"fas fa-external-link-alt"}},
  shortcuts: undefined,
  prompt: {"copyButton":"复制","copySuccess":"复制成功","copyError":"复制失败"},
  sourcePath: {"js":"js","css":"css","images":"images"},
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 6.3.0"></head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner header-inner--height header-inner--bgcolor"><nav class="header-nav header-nav--sticky"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">首页</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/archives/"><span class="header-nav-menu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-menu-item__text">归档</span></a></div></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content" id="content"><!-- Just used to judge whether it is an article page--><div id="is-post"></div><div class="post"><header class="post-header"><h1 class="post-title">ML_learning</h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2023-12-26</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2023-12-26</span></span></div></header><div class="post-body">
        <h2 id="Approaching-Almost-Any-Machine-Learning-Problem"   >
          <a href="#Approaching-Almost-Any-Machine-Learning-Problem" class="heading-link"><i class="fas fa-link"></i></a><a href="#Approaching-Almost-Any-Machine-Learning-Problem" class="headerlink" title="Approaching (Almost) Any Machine Learning Problem"></a>Approaching (Almost) Any Machine Learning Problem</h2>
      <blockquote>
<p>解决几乎所有机器学习问题 很有气势！</p>
</blockquote>

        <h3 id="1-环境"   >
          <a href="#1-环境" class="heading-link"><i class="fas fa-link"></i></a><a href="#1-环境" class="headerlink" title="1 环境"></a>1 环境</h3>
      <p>安装环境 要指定Python版本 pip的版本 【很重要】</p>
<p>安装软件包 使用pip 【conda库不全】</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n environment_name python=3.7.6 pip=2.23</span><br></pre></td></tr></table></div></figure>

<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda env create -f environment.yml # 现有的ML依赖包 网卡就上PyPI上下载 然后 pip install xxxx.whl</span><br></pre></td></tr></table></div></figure>


        <h3 id="2-无监督-和-有监督"   >
          <a href="#2-无监督-和-有监督" class="heading-link"><i class="fas fa-link"></i></a><a href="#2-无监督-和-有监督" class="headerlink" title="2 无监督 和 有监督"></a>2 无监督 和 有监督</h3>
      <p>有监督，可以确定地知道目标变量。如果目标变量是 一个数（浮点数） 【回归问题，预测房价】 多个组【分类问题，猫和狗区分】。</p>
<p>无监督，数据集没有明确的目标变量。聚类方法，可以把数据分为 几类【人来判断几个最合适】</p>
<ul>
<li>分解技术 如主成分分析 <strong>t-分布随机邻域嵌入（t-SNE）</strong>等找到几个可行的类别</li>
</ul>
<blockquote>
<p>t-SNE（t-Distributed Stochastic Neighbor Embedding)是一种无监督的、随机统计方法。用于<strong>高维数据的嵌入和可视化，降维 筛选有效特征</strong></p>
<p>t-SNE 工作原理可以分为两个步骤：</p>
<ol>
<li><p>高维相似度：在原始高维空间中，每一点对其他所有点的距离可以通过高斯分布来衡量，形成一种概率分布，表示一个点选择其他点作为其邻居的概率。</p>
</li>
<li><p>低维相似度：在低维空间中，每一点对其他所有点的距离也可以通过概率分布来衡量，但这个概率分布遵循的是<code>t分布</code>。</p>
</li>
</ol>
<p>t-SNE的目标是使得高维空间和低维空间中的两种概率分布<code>尽可能一致</code>，也就是让相似的对象在低维空间的投影仍然相似。</p>
<p>在机器学习中用于理解模型的内部工作机制，把人无法理解的高维度关系 ‘<code>尽量无损的</code>’ 移到平面或三维空间来，方便分析。</p>
</blockquote>
<p>用无监督方法处理有标签的数据，例子t-SNE 聚类手写数字，可以查看数据有没有明显的聚集趋势【<strong>分类边界</strong>】：</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">在google colab上不用考虑环境配置的问题，下载速度快，依赖包全且新</span></span><br><span class="line"><span class="string">Original file is located at</span></span><br><span class="line"><span class="string">    https://colab.research.google.com/drive/1PsPPT_Cnywzfl_7WXK200yA2-LEYQwZ8</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Commented out IPython magic to ensure Python compatibility.</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> manifold</span><br><span class="line"><span class="comment"># %matplotlib inline</span></span><br><span class="line"></span><br><span class="line">data = datasets.fetch_openml(<span class="string">&#x27;mnist_784&#x27;</span>, version=<span class="number">1</span>, return_X_y=<span class="literal">True</span>)</span><br><span class="line">pixel_values, targets = data <span class="comment"># 把结果 分为图像和标签两部分</span></span><br><span class="line">targets = targets.astype(<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查基本格式</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(pixel_values))</span><br><span class="line"><span class="built_in">print</span>(pixel_values.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看单个图片</span></span><br><span class="line">single_image = pixel_values.iloc[<span class="number">1</span>].to_numpy().reshape(<span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">plt.imshow(single_image, cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立一个两维的t-SNE实例</span></span><br><span class="line">tsne = manifold.TSNE(n_components=<span class="number">2</span>, random_state=<span class="number">42</span>) </span><br><span class="line"><span class="comment"># pd格式不支持直接切片语法，需要转换为numpy或tsne.fit_transform(pixel_values.iloc[:3000, :])</span></span><br><span class="line">pixel_values_np = pixel_values.to_numpy()</span><br><span class="line">transformed_data = tsne.fit_transform(pixel_values_np[:<span class="number">3000</span>, :]) <span class="comment"># 降维并拟合</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个包含transformed_data,targets[:3000]的数据框，并把这个数据框的列名分别命名为&quot;x&quot;,&quot;y&quot;和&quot;targets&quot;，</span></span><br><span class="line"><span class="comment"># 然后将&quot;targets&quot;列的数据类型转换成整数类型。</span></span><br><span class="line">tsne_df = pd.DataFrame(np.column_stack((transformed_data, targets[:<span class="number">3000</span>])),</span><br><span class="line">                       columns=[<span class="string">&quot;x&quot;</span>, <span class="string">&quot;y&quot;</span>, <span class="string">&quot;targets&quot;</span>])</span><br><span class="line">tsne_df.loc[:, <span class="string">&quot;targets&quot;</span>] = tsne_df.targets.astype(<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘图，观察聚类效果</span></span><br><span class="line">grid = sns.FacetGrid(tsne_df, hue=<span class="string">&quot;targets&quot;</span>, height=<span class="number">8</span>)</span><br><span class="line">grid.<span class="built_in">map</span>(plt.scatter, <span class="string">&quot;x&quot;</span>, <span class="string">&quot;y&quot;</span>).add_legend()</span><br></pre></td></tr></table></div></figure>

<img src="./ML-learning.assets/手写数字t-SNE聚类.png" alt="手写数字t-SNE聚类" style="zoom:67%;" />

<p>可以观察到，使用t-SNE即使把784维度【像素个数】的图片降维到二维平面，依然能清晰观察到同一种数字靠拢在一起。9和5混在一起正说明两者的相似性。</p>

        <h3 id="3-交叉检验"   >
          <a href="#3-交叉检验" class="heading-link"><i class="fas fa-link"></i></a><a href="#3-交叉检验" class="headerlink" title="3 交叉检验"></a>3 交叉检验</h3>
      <p>交叉检验是构建机器学习模型的一个步骤，它帮助模型准确拟合数据，同时避免过拟合。</p>
<blockquote>
<p>把k折交叉验证想象成魔术表演。魔术师喜欢纸牌魔术，一堆纸牌（不要大小王，50张）就是数据集。</p>
<p>第一步，魔术师先纸牌平均分成k份，假如是5折交叉验证，那纸牌被平均地分成5堆，每堆10张。</p>
<p>接着，魔术开始。他选择4堆纸牌（也就是80%的数据）作为他的”训练集”，用以练习魔术，看看什么样的手法可以让观众最惊讶。剩下那一堆（就是20%的数据）用作”测试集”，在这里他会<strong>真正地</strong>表演他的魔术，看看观众的反应。</p>
<p>魔术师一共做5次特别的表演。每次，他都会更换一下他的训练集和测试集（也就是把前面已经用过的测试集放回去，再取出另一堆牌作为新的测试集），这样他就可以确保他的所有纸牌都被用到，也就是<strong>所有的数据都有机会被测试</strong>。</p>
<p>整个过程里，魔术师会记住哪些魔术最令观众惊奇，也就是哪个模型的性能最好。然后，他就会把这个最好的方法，用在他之后的全部魔术表演之中。</p>
</blockquote>
<p><strong>过拟合</strong> 随着不断训练，训练集和测试集的损失都会减少。但是，在某个时刻，测试的损失会达到<code>最小值</code>，之后，即使训练损失进一步减少，测试的损失却开始增大。</p>
<blockquote>
<p>注: 测试的损失 指向 在实际生产环境中的损失。因为，实际使用的数据 和 测试数据 对模型来说都是第一次见。过犹不及，应追求在测试集上损失最小 而不是训练集上的损失最小，毕竟模型是要拿出去用的，不是放在家里展览的。</p>
</blockquote>
<img src="./ML-learning.assets/过拟合.png" alt="过拟合" style="zoom: 67%;" />



<p>例子，红酒质量数据，以下是对决策树单次训练的结果【训练集准度，测试集准度】：</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 手动下载数据并解压 https://archive.ics.uci.edu/dataset/186/wine+quality、</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="comment"># csv 默认是逗号，但是这里是分号，需要显式指明</span></span><br><span class="line">df = pd.read_csv(<span class="string">&quot;/content/drive/MyDrive/AnyML/winequality-red.csv&quot;</span>,sep=<span class="string">&quot;;&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(df.columns) <span class="comment"># 检查列名</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 一个映射字典，用于将质量值从 0 到 5 进行映射</span></span><br><span class="line">quality_mapping = &#123;</span><br><span class="line"> <span class="number">3</span>: <span class="number">0</span>,</span><br><span class="line"> <span class="number">4</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="number">5</span>: <span class="number">2</span>,</span><br><span class="line"> <span class="number">6</span>: <span class="number">3</span>,</span><br><span class="line"> <span class="number">7</span>: <span class="number">4</span>,</span><br><span class="line"> <span class="number">8</span>: <span class="number">5</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可使用 pandas 的 map 函数以及任何字典，来转换给定列中的值为字典中的值</span></span><br><span class="line">df.loc[:, <span class="string">&quot;quality&quot;</span>] = df.quality.<span class="built_in">map</span>(quality_mapping)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打乱df的行顺序并重新设置连续的索引</span></span><br><span class="line">df = df.sample(frac=<span class="number">1</span>).reset_index(drop=<span class="literal">True</span>) <span class="comment"># frac 采样占比</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 选取前 1000 行作为训练数据</span></span><br><span class="line">df_train = df.head(<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选取最后的 599 行作为测试/验证数据</span></span><br><span class="line">df_test = df.tail(<span class="number">599</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从 scikit-learn 导入需要的模块</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化一个决策树分类器，设置最大深度为 3</span></span><br><span class="line">clf = tree.DecisionTreeClassifier(max_depth=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择想要训练模型的列作为模型的特征</span></span><br><span class="line">cols = [<span class="string">&#x27;fixed acidity&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;volatile acidity&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;citric acid&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;residual sugar&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;chlorides&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;free sulfur dioxide&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;total sulfur dioxide&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;density&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;pH&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;sulphates&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;alcohol&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用之前映射的质量以及提供的特征来训练模型</span></span><br><span class="line">clf.fit(df_train[cols], df_train.quality)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在训练集上生成预测</span></span><br><span class="line">train_predictions = clf.predict(df_train[cols])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试集上生成预测</span></span><br><span class="line">test_predictions = clf.predict(df_test[cols])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算训练数据集上预测的准确度</span></span><br><span class="line">train_accuracy = metrics.accuracy_score(</span><br><span class="line"> df_train.quality, train_predictions</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算测试数据集上预测的准确度</span></span><br><span class="line">test_accuracy = metrics.accuracy_score(</span><br><span class="line"> df_test.quality, test_predictions</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(train_accuracy,test_accuracy) <span class="comment">#0.588 0.5659432387312187</span></span><br><span class="line"></span><br></pre></td></tr></table></div></figure>



<p>继续，对决策树使用<strong>不同的深度</strong>，研究【训练集准度，测试集准度】的变化，注意到末尾的图示，当我不断增大树的深度必然导致过度拟合（决策树的复杂度&gt;&gt;训练数据的所需），测试数据的准度到达顶峰开始下滑。</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 注意：这段代码在 Jupyter 笔记本中编写</span></span><br><span class="line"><span class="comment"># 导入 scikit-learn 的 tree 和 metrics</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="comment"># 导入 matplotlib 和 seaborn</span></span><br><span class="line"><span class="comment"># 用于绘图</span></span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置全局标签文本的大小</span></span><br><span class="line">matplotlib.rc(<span class="string">&#x27;xtick&#x27;</span>, labelsize=<span class="number">20</span>)</span><br><span class="line">matplotlib.rc(<span class="string">&#x27;ytick&#x27;</span>, labelsize=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 确保图表直接在笔记本内显示</span></span><br><span class="line"><span class="comment"># %matplotlib inline</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化用于存储训练和测试准确度的列表</span></span><br><span class="line"><span class="comment"># 我们从 50% 的准确度开始</span></span><br><span class="line">train_accuracies = [<span class="number">0.5</span>]</span><br><span class="line">test_accuracies = [<span class="number">0.5</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历几个不同的树深度值</span></span><br><span class="line"><span class="keyword">for</span> depth <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">25</span>):</span><br><span class="line">    <span class="comment"># 初始化模型</span></span><br><span class="line">    clf = tree.DecisionTreeClassifier(max_depth=depth)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 选择用于训练的列/特征</span></span><br><span class="line">    cols = [</span><br><span class="line">        <span class="string">&#x27;fixed acidity&#x27;</span>, <span class="string">&#x27;volatile acidity&#x27;</span>, <span class="string">&#x27;citric acid&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;residual sugar&#x27;</span>, <span class="string">&#x27;chlorides&#x27;</span>, <span class="string">&#x27;free sulfur dioxide&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;total sulfur dioxide&#x27;</span>, <span class="string">&#x27;density&#x27;</span>, <span class="string">&#x27;pH&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;sulphates&#x27;</span>, <span class="string">&#x27;alcohol&#x27;</span></span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在给定特征上拟合模型</span></span><br><span class="line">    clf.fit(df_train[cols], df_train.quality)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建训练和测试预测</span></span><br><span class="line">    train_predictions = clf.predict(df_train[cols])</span><br><span class="line">    test_predictions = clf.predict(df_test[cols])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算训练和测试准确度</span></span><br><span class="line">    train_accuracy = metrics.accuracy_score(</span><br><span class="line">        df_train.quality, train_predictions</span><br><span class="line">    )</span><br><span class="line">    test_accuracy = metrics.accuracy_score(</span><br><span class="line">        df_test.quality, test_predictions</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 添加准确度到列表</span></span><br><span class="line">    train_accuracies.append(train_accuracy)</span><br><span class="line">    test_accuracies.append(test_accuracy)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 matplotlib 和 seaborn 创建两个图</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">5</span>))</span><br><span class="line">sns.set_style(<span class="string">&quot;whitegrid&quot;</span>)</span><br><span class="line">plt.plot(train_accuracies, label=<span class="string">&quot;train accuracy&quot;</span>)</span><br><span class="line">plt.plot(test_accuracies, label=<span class="string">&quot;test accuracy&quot;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&quot;upper left&quot;</span>, prop=&#123;<span class="string">&#x27;size&#x27;</span>: <span class="number">15</span>&#125;)</span><br><span class="line">plt.xticks(<span class="built_in">range</span>(<span class="number">0</span>, <span class="number">26</span>, <span class="number">5</span>))</span><br><span class="line">plt.xlabel(<span class="string">&quot;max_depth&quot;</span>, size=<span class="number">20</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;accuracy&quot;</span>, size=<span class="number">20</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></div></figure>

<p><img src="/./ML-learning.assets/%E8%BF%87%E6%8B%9F%E5%90%88-1700019734922-3.png" alt="过拟合"></p>
<blockquote>
<p>注：以上可见 选择决策树深度为6或7是合适的，之后虽然训练集精度不断提升，但测试的结果不升反降，这就是过拟合。当然这个结果和你选择特征的质量 数量直接相关。</p>
</blockquote>
<p>奥卡姆剃刀，简单说，就是不要试图把可以用简单得多的方法解决的事情复杂化。<code>如无必要，勿增实体</code>，这个原则被用在理论选择中，当有多个可以解释某个现象的理论存在时，应优先选择最简单（即需假设最少的）那一种。</p>
<p>交叉验证中<code>每一份都会被当成训练集，也会被当成测试集</code>，以此来降低<code>偶然切分</code>导致的误判。</p>
<ul>
<li><p>k 折交叉检验 小规模【小于100万】 回归问题 python代码 <code>num_bins = int(np.floor(1 + np.log2(len(data))))</code><br>$$<br>k &#x3D; log_2(N)+1<br>$$</p>
</li>
<li><p>分层k折交叉检验  是<strong>标准分类</strong>问题，就盲目选择分层 k 折交叉检验</p>
</li>
<li><p><strong>暂留交叉检验</strong>（<strong>hold-out set</strong>） 应对大量的数据，模型推理【训练过程】是一个耗时的过程时 只保留一份用于测试</p>
</li>
<li><p>留一交叉检验</p>
</li>
<li><p>分组 k 折交叉检验</p>
</li>
</ul>
<p>除了分层 k 折交叉检验之外，我们可以在<strong>回归问题</strong>上使用上述所有交叉检验技术。</p>
<p>例1，<strong>k折交叉验证</strong>的简单例子，目标添加一个新的列kfold，记录每行数据属于哪一个折<code>(0~k-1)</code>：</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入 pandas 和 scikit-learn 的 model_selection 模块</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    df = pd.read_csv(<span class="string">&quot;train.csv&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 我们创建一个名为 kfold 的新列，并用 -1 填充</span></span><br><span class="line">    df[<span class="string">&quot;kfold&quot;</span>] = -<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 接下来的步骤是随机打乱数据的行</span></span><br><span class="line">    df = df.sample(frac=<span class="number">1</span>).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 从 model_selection 模块初始化 kfold 类</span></span><br><span class="line">    kf = model_selection.KFold(n_splits=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 填充新的 kfold 列（enumerate的作用是返回一个迭代器）</span></span><br><span class="line">    <span class="keyword">for</span> fold, (trn_, val_) <span class="keyword">in</span> <span class="built_in">enumerate</span>(kf.split(X=df)):</span><br><span class="line">        df.loc[val_, <span class="string">&#x27;kfold&#x27;</span>] = fold</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存带有 kfold 列的新 CSV 文件</span></span><br><span class="line">    df.to_csv(<span class="string">&quot;train_folds.csv&quot;</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>注：拿扑克牌举例，首先洗牌，平均切分为<code>五</code>份【之间互不相连】，每一份加入一个标签（<code>0 1 2 3 4</code>），每次留一份做验证。</p>
</blockquote>
<p>例2：<strong>分层 k 折交叉检验</strong>，面对偏斜的二元分类数据集，正样本90%，负样本只占 10%，那么你就不应该使用随机 k 折交叉。只需将 model_selection.KFold 更改为 model_selection.StratifiedKFold ，并在 kf.split(…) 函数中指定要分层的目标列。我们假设 CSV 数据集有一列名为 “target”[<strong>即标签值</strong>] ，并且是一个分类问题。</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入 pandas 和 scikit-learn 的 model_selection 模块</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 训练数据保存在名为 train.csv 的 CSV 文件中</span></span><br><span class="line">    df = pd.read_csv(<span class="string">&quot;train.csv&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 添加一个新列 kfold，并用 -1 初始化</span></span><br><span class="line">    df[<span class="string">&quot;kfold&quot;</span>] = -<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 随机打乱数据行</span></span><br><span class="line">    df = df.sample(frac=<span class="number">1</span>).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取目标变量</span></span><br><span class="line">    y = df.target.values</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化 StratifiedKFold 类，设置折数（folds）为 5</span></span><br><span class="line">    kf = model_selection.StratifiedKFold(n_splits=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用 StratifiedKFold 对象的 split 方法来获取训练和验证索引</span></span><br><span class="line">    <span class="keyword">for</span> f, (t_, v_) <span class="keyword">in</span> <span class="built_in">enumerate</span>(kf.split(X=df, y=y)):</span><br><span class="line">        df.loc[v_, <span class="string">&#x27;kfold&#x27;</span>] = f</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存包含 kfold 列的新 CSV 文件</span></span><br><span class="line">    df.to_csv(<span class="string">&quot;train_folds.csv&quot;</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>注：<code>StratifiedKFold</code>相较于普通的<code>KFold</code>，会保证每一份的数据集的类别分布和整个样本集的类别分布是一样的，这在处理类别不平衡问题时会更有优势。在测试<code>PDF</code>不平衡数据的时候，我就没有考虑分层的情况，可能有一些问题。</p>
</blockquote>
<p>例3，回归数据的 分层k折交叉检验 使用Sturge规则计算bins的值</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># stratified-kfold for regression</span></span><br><span class="line"><span class="comment"># 为回归问题进行分层K-折交叉验证</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入需要的库</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建分折（folds）的函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_folds</span>(<span class="params">data</span>):</span><br><span class="line">    <span class="comment"># 创建一个新列叫做kfold，并用-1来填充</span></span><br><span class="line">    data[<span class="string">&quot;kfold&quot;</span>] = -<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 随机打乱数据的行</span></span><br><span class="line">    data = data.sample(frac=<span class="number">1</span>).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用Sturge规则计算bin的数量 </span></span><br><span class="line">    num_bins = <span class="built_in">int</span>(np.floor(<span class="number">1</span> + np.log2(<span class="built_in">len</span>(data)))) <span class="comment"># 计算合适的k，而不是人去指定</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用pandas的cut函数进行目标变量（target）的分箱</span></span><br><span class="line">    data.loc[:, <span class="string">&quot;bins&quot;</span>] = pd.cut(</span><br><span class="line">        data[<span class="string">&quot;target&quot;</span>], bins=num_bins, labels=<span class="literal">False</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化StratifiedKFold类</span></span><br><span class="line">    kf = model_selection.StratifiedKFold(n_splits=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 填充新的kfold列</span></span><br><span class="line">    <span class="comment"># 注意：我们使用的是bins而不是实际的目标变量（target）！</span></span><br><span class="line">    <span class="keyword">for</span> f, (t_, v_) <span class="keyword">in</span> <span class="built_in">enumerate</span>(kf.split(X=data, y=data.bins.values)):</span><br><span class="line">        data.loc[v_, <span class="string">&#x27;kfold&#x27;</span>] = f</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 删除bins列</span></span><br><span class="line">    data = data.drop(<span class="string">&quot;bins&quot;</span>, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回包含folds的数据</span></span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 主程序开始</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 创建一个带有15000个样本、100个特征和1个目标变量的样本数据集</span></span><br><span class="line">    X, y = datasets.make_regression(</span><br><span class="line">        n_samples=<span class="number">15000</span>, n_features=<span class="number">100</span>, n_targets=<span class="number">1</span></span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># 使用numpy数组创建一个数据框</span></span><br><span class="line">    df = pd.DataFrame(</span><br><span class="line">        X,</span><br><span class="line">        columns=[<span class="string">f&quot;f_<span class="subst">&#123;i&#125;</span>&quot;</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(X.shape[<span class="number">1</span>])]</span><br><span class="line">    )</span><br><span class="line">    df.loc[:, <span class="string">&quot;target&quot;</span>] = y</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建folds</span></span><br><span class="line">    df = create_folds(df)</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>注：bins 目的是把回归问题中连续的（实数）预测值，放到几个桶里（整数），把浮点数预测变成分类问题，然后分层k折。</p>
</blockquote>
<p>课后练习：</p>
<p>假设我们希望建立一个模型，从患者的皮肤图像中检测出皮肤癌。任务是建一个二元分类器，该分类器接收输入图像并预测其良性或恶性的概率。在这类数据集中，训练数据集中<strong>可能有同一患者的多张图像</strong>。因此，要在这里建立一个良好的交叉检验系统，必须有分层的 k 折交叉检验，但也必须确保训练数据中的患者不会出现在验证数据中。幸运的是，scikit-learn 提供了一种称为 GroupKFold 的交叉检验类型。 在这里，患者可以被视为组。 但遗憾的是，scikit-learn 无法将 GroupKFold 与 StratifiedKFold 结合起来。</p>
<p>如何手动实现呢？</p>
<p>要手动实plement一个Grouped Stratified K-Fold Cross Validation，需要参考scikit-learn提供的GroupKFold和StratifiedKFold的API。以下是一个可能的步骤：</p>
<p>步骤如下：</p>
<ol>
<li><p>首先，我们对所有数据进行分层，确保每个类在每个折叠中具有相等的比例。 StratifiedKFold 可以实现这个目标。可以使用Scikit-Learn的<code>StratifiedKFold</code>函数来实现，例：</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedKFold</span><br><span class="line">skf = StratifiedKFold(n_splits=K, shuffle=<span class="literal">True</span>, random_state=random_state)</span><br></pre></td></tr></table></div></figure>
</li>
<li><p>然后，对每个分层后的折叠，我们都遵从 group 的要求，也就是说，一个折叠内的所有样本都来自于同一组（这里的组可以理解为来自同一患者的所有样本）。</p>
</li>
<li><p>对于每一个StratifiedKFold的数据分区，我们进一步按照各组ID（即患者ID）进行分组，确保同一个患者的数据不会出现在训练集和验证集的交叉分布。</p>
</li>
</ol>
<p>以下是一个概念上的样例代码：</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedKFold</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设X是输入数据，y是相应的标签，groups是病人的ID</span></span><br><span class="line">X = ...</span><br><span class="line">y = ...</span><br><span class="line">groups = ...</span><br><span class="line"></span><br><span class="line">strat_kfold = StratifiedKFold(n_splits=K, shuffle=<span class="literal">True</span>, random_state=random_state)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> train_index, test_index <span class="keyword">in</span> strat_kfold.split(X, y):</span><br><span class="line">    train_groups = groups[train_index]</span><br><span class="line">    unique_train_groups = np.unique(train_groups)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 检查在测试集中是否又与训练集中相同的组</span></span><br><span class="line">    overlap = np.isin(groups[test_index], unique_train_groups)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果有重叠，我们需要将其调整到与训练集相同的组</span></span><br><span class="line">    <span class="keyword">while</span> np.<span class="built_in">any</span>(overlap):</span><br><span class="line">        <span class="comment"># 从训练集中移除这些组</span></span><br><span class="line">        mask = np.isin(train_groups, groups[test_index][overlap])</span><br><span class="line">        train_index = train_index[~mask]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 并将它们添加到测试集</span></span><br><span class="line">        test_index = np.concatenate([test_index, train_index[mask]])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更新组的列表和重叠情况</span></span><br><span class="line">        train_groups = groups[train_index]</span><br><span class="line">        unique_train_groups = np.unique(train_groups)</span><br><span class="line">        overlap = np.isin(groups[test_index], unique_train_groups)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 现在train_index和test_index分别表示了分层且基于组的训练和测试集</span></span><br><span class="line">    X_train, X_test = X[train_index], X[test_index]</span><br><span class="line">    y_train, y_test = y[train_index], y[test_index]</span><br></pre></td></tr></table></div></figure>

<p>这就是所有的步骤。这个方法的主要目标是：对类别进行分层，同时在折叠中维持组的完整性。这使得我们能够在训练和测试模型时大大减少过拟合或漏泄的可能性。</p>

        <h3 id="4-评价指标"   >
          <a href="#4-评价指标" class="heading-link"><i class="fas fa-link"></i></a><a href="#4-评价指标" class="headerlink" title="4 评价指标"></a>4 评价指标</h3>
      <ol>
<li>把手头模型想象为一位”侦探”，任务是找出”坏人”（即恶意文件）：<ol>
<li>真阳性（True Positive）: 侦探认为某人是坏人，他真的是坏人。也就是说，模型成功地识别出了一个恶意文件。</li>
<li>假阳性（False Positive）: 侦探认为某人是坏人，但实际上他是好人。模型错误地把一个正常的文件认为是恶意文件。【<strong>误报</strong>】</li>
<li>真阴性（True Negative）: 侦探认为某人是好人，且他真的是好人。模型成功地识别出了一个非恶意文件。</li>
<li>假阴性（False Negative）: 侦探认为某人是好人，但实际上他是坏人。模型错误地把一个恶意文件认为是正常文件。【<strong>漏报</strong>】</li>
</ol>
</li>
<li>二元分类指标中，当正负样本数量相等时，我们通常使用准确率、精确率、召回率和 F1。<ol>
<li>精确率 真阳性&#x2F;（真阳+假阳） 真恶意文件&#x2F;(真恶意+误判恶意) 在所有预测为恶意中，真正找对几个</li>
<li>Recall率 真阳&#x2F;（真阳+假阴） 真恶意文件&#x2F;(真恶意+误判良性) 在所有恶意文件中找回来多少真恶意</li>
</ol>
</li>
</ol>
<blockquote>
<p>让我们将这些概念变得更加通俗和生动一些。继续侦探寻找”坏人”的游戏，我们可以将精确率、召回率和F1分数的定义以此类推。</p>
<ol>
<li><p>精确率：以为自己找到了一些”坏人”，但他们是”好人”。精确率其实是指你认定的”坏人”中，真正是”坏人”的比例。比如，你找到了10个你认为的”坏人”，但实际上只有5个是真正的”坏人”，那么你的精确率就是5&#x2F;10&#x3D;50%。</p>
</li>
<li><p>召回率：假设在所有的人中，实际上有20个”坏人”。召回率是说你能找出多少真正的”坏人”。比如，你找到了10个你认为的”坏人”，并且这10个人都真的是”坏人”，但实际上总共有20个”坏人”，那么你的召回率就是10&#x2F;20&#x3D;50%。</p>
</li>
<li><p>F1: 这是精确率与召回率的平均，可以理解为两者的平均成绩。就像你参加两个考试，一个测试精确度（你找出的‘坏人’有多少是真的’坏人’），一个测试召回率（你找到的’坏人’占总’坏人’的多少），而F1就是这两个考试的平均分。</p>
</li>
</ol>
<p>准确率：假设一共有100个人，其中有20个”坏人”，80个”好人”，准确率就是，在这100个人中正确判断出”好人”和”坏人”的比例。比如，你判断出了15个”坏人”（实际上都是”坏人”）和70个”好人”（实际上都是”好人”），那么你的准确率就是(15+70)&#x2F;100&#x3D;85%。</p>
</blockquote>
<p>当让一个8岁的孩子理解这些机器学习的概念时，我们可以试图用更简单、更具象的例子来解释。</p>
<ol>
<li><p>AUC（Area Under the Curve）：想象一场足球比赛，我们的目标是预测一队是否能赢得比赛。每一队在开始比赛之前都会评估对手，判断他们是否会赢。现在，把这种判断的正确率画在纸上，如果我们判断得越准，那么我们在纸上的画就会越靠上。AUC就是这个图中我们的预测画出的部分与整个纸面的占比。<code>如果足球队判断的总是非常精确，那么我们的图就会占据大部分的空间，这就意味着AUC的值是很大的；如果判断的不准，那么我们所占的空间就会很小，AUC的值就小。</code></p>
</li>
<li><p>对数损失（Log loss）：让我们考虑一个掷硬币的游戏，你要猜测是正面还是反面。如果你非常确定硬币会落在正面，但结果却落在了反面，那你会觉得自己猜错了，并且错得离真实结果差很远。这就是对数损失，它测量的是你的预测和真实答案之间的差距。越接近真实的答案，对数损失就越小，也就表明你的预测越准确。</p>
</li>
<li><p>k 精确率（P@k）：假设我让你从一堆宝石中挑选出真正的宝石。你的挑选能力就像一个预测机器，试图预测哪些是真宝石。P@k就是看你挑选的前k个物品中，真正的宝石有多少。例如，如果你挑选出前5（假设k&#x3D;5）个中有3个真正的宝石，那么你的P@5就是3&#x2F;5&#x3D;0.6。</p>
</li>
<li><p>k 平均精率（AP@k）：这个和挑选宝石的游戏一样，只不过这次我们关心的不只是你挑选的宝石的总数，还关心你挑选的顺序。如果你最先挑出来的都是真宝石，那么你的评分就会更高。这就是AP@k，它考虑了你挑选的顺序和挑选的宝石的数量。</p>
</li>
<li><p>k 均值平均精确率（MAP@k）：还是那个宝石游戏，但这次我们更关心整体表现。如果你参与了3次挑选，每次挑选的表现（AP@k）都会记录下来，然后我们会求这3次的平均表现。这就是MAP@k，它帮助我们了解你在挑选宝石这个游戏中的整体表现。</p>
</li>
</ol>
<p><strong>回归问题 误差 :</strong></p>
<p>首先我们把模型预测错误理解成我们在打靶的时候，箭偏离了我们的目标。</p>
<ol>
<li><strong>平均绝对误差 (MAE)</strong>: 这就像你每次打箭偏离的距离，有时偏左，有时偏右，但我们只管看偏离目标的绝对距离，不管偏左还是偏右，然后我们计算你每次打箭的平均偏离距离。</li>
<li><strong>均方误差 (MSE)</strong>: 这像你把每次射箭偏移的距离的平方求和然后除以箭的数量。因为我们不仅看你偏离目标的距离，我们还考虑你偏离的幅度。例如，我们更重视你偏离的距离较长的错误。</li>
<li><strong>均方根误差 (RMSE)</strong>: 这就像你首先计算每次你射箭离目标的距离的平方，然后求全部的平均值，最后开平方，这样可以更真实地反映你的平均表现，因为它考虑到了大的偏离比小的偏离更严重。</li>
<li><strong>均方根对数误差 (RMSLE)</strong>: 这就像RMSE, 但在比较你的预测和真实值的偏离度的时候，我们并不是直接比较这两个值，而是比较它们的对数。这样就意味着我们对预测值与真实值差别的比例更感兴趣，而不是其绝对差别。例如，预测100元而真实值为200元的错误和预测1000元而真实值为2000元的错误被看作是相同的。</li>
<li><strong>平均百分比误差 (MPE)</strong>: 这就像每次你射箭，我们不仅仅看你离目标偏多少距离，还要看这个距离相对于目标的总距离的比例是多少，并且是有方向的，偏左和偏右是有区别的。</li>
<li><strong>平均绝对百分比误差 (MAPE)</strong>: 这就是对每次你射箭偏离目标的距离的比例进行平均，偏左和偏右看作一样罪大恶极。</li>
<li><strong>R2</strong>: 这就像是你的瞄准器相比于一个完全乱射的人，表现得有多好。如果你的瞄准器完全正确，R2就是1；如果你的瞄准器像一个完全随机射箭的人，R2就是0；如果你的瞄准器做得比随机射箭的人还糟，R2就可能是负的。</li>
</ol>
<p>总的来说，所有这些指标都能以不同的方式告诉我们预测与实际结果之间的差距大小，偏离方向以及偏离的频率等信息，帮助我们判断模型的预测性能。</p>
<blockquote>
<p>注：简单理解，会用即可。</p>
</blockquote>
<p>在二元分类的情况下，可采用类似下面的方法。<br>$$<br>Prediction&#x3D;Probability &gt;&#x3D; Threshold<br>$$<br>预测是一个只包含二元变量的新列表。如果概率大于或等于给定的阈值，则预测中的一项为 1，否则为 0。你猜怎么着，你可以<strong>使用 ROC 曲线来选择这个阈值</strong>！ROC 曲线会告诉您阈值对假阳性率和真阳性率的影响，进而影响假阳性和真阳性。</p>
<p>为了选择最适合您的问题和数据集的阈值，可根据验证集结果选择阈值：当你有一个初步的模型训练完成后，可以在验证集上进行试验，具体就是<strong>尝试不同的阈值</strong>，观察不同阈值下模型的精确率、召回率、F1 Score（精确率和召回率的调和平均值）以及ROC曲线下的面积(AUC-ROC)【<code>多方面的考察</code>】，根据你的目标选择最优的阈值。</p>
<blockquote>
<p>这也不难。那什么难？什么都不难。机器学习很简单。</p>
</blockquote>

        <h3 id="5-模型的结构组织"   >
          <a href="#5-模型的结构组织" class="heading-link"><i class="fas fa-link"></i></a><a href="#5-模型的结构组织" class="headerlink" title="5 模型的结构组织"></a>5 模型的结构组织</h3>
      <ul>
<li>input&#x2F;  所有输入文件和数据 NLP的embeddings 图像<ul>
<li>train.csv</li>
<li>test.csv</li>
</ul>
</li>
<li>src&#x2F; 所有相关的Python脚本 <code>*.py</code><ul>
<li>create_folds.py</li>
<li>train.py</li>
<li>inference.py</li>
<li>models.py</li>
<li>config.py 存放必要的全局配置</li>
<li>model_dispatcher.py 将调度我们的模型到训练脚本中</li>
</ul>
</li>
<li>models&#x2F; 所有训练过的模型<ul>
<li>model_rf.bin</li>
<li>model_et.bin</li>
</ul>
</li>
<li>notebooks&#x2F; 所有 jupyter notebook（即任何 <code>*.ipynb</code> 文件）都存储在笔记本文件夹中。<ul>
<li>exploration.ipynb</li>
<li>check_data.ipynb</li>
</ul>
</li>
<li>README.md 描述您的项目，并写明如何训练模型或在生产环境中使用。</li>
<li>LICENSE 包含项目的许可证，如 MIT、Apache 等。</li>
</ul>
<blockquote>
<p>注：src 部分的切分很合理，尤其是模型调度文件和全局配置。</p>
</blockquote>
<p>例子，识别手写数字：</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── LICENSE</span><br><span class="line">├── README.md</span><br><span class="line">├── input</span><br><span class="line">│   ├── mnist_test.csv</span><br><span class="line">│   ├── mnist_train.csv</span><br><span class="line">│   └── mnist_train_folds.csv</span><br><span class="line">├── models</span><br><span class="line">│   ├── dt_0.bin</span><br><span class="line">│   └── rf_0.bin</span><br><span class="line">├── notebooks</span><br><span class="line">└── src</span><br><span class="line">    ├── __pycache__</span><br><span class="line">    ├── config.py</span><br><span class="line">    ├── create_folds.py</span><br><span class="line">    ├── model_dispatcher.py</span><br><span class="line">    ├── run.sh</span><br><span class="line">    └── train.py</span><br></pre></td></tr></table></div></figure>




        <h3 id="6-处理分类变量"   >
          <a href="#6-处理分类变量" class="heading-link"><i class="fas fa-link"></i></a><a href="#6-处理分类变量" class="headerlink" title="6 处理分类变量"></a>6 处理分类变量</h3>
      <p>分类变量 <strong>离散</strong>变量 名义变量 只可以取有限数量的变量 【有限集合】</p>
<blockquote>
<p>注：分类变量可以是目标变量，也可以是特征变量。相反，目标变量也可以是分类变量，也可以是连续变量。</p>
</blockquote>
<ul>
<li><p>无序 男性或女性 猫或狗</p>
</li>
<li><p>有序 低中高【有次序 有强度 程度的变化】</p>
<ul>
<li><p>标签变为数字编码  - 决策树 - 随机森林 - 提升树 - 或任何一种提升树模型 - XGBoost - GBM - LightGBM</p>
<blockquote>
<p>注：这样做也是有风险的，例如 把低中高编码为 1 2 3，这样<strong>默认了结果之间是等距离</strong>的，这符合现实吗？</p>
</blockquote>
</li>
<li><p>【稀疏矩阵】数字扩展为 二进制（一个特征变为多个特征） - 线性模型、支持向量机或神经网络期待标准化数据</p>
</li>
<li><p>one-hot</p>
<blockquote>
<p>注：在用基于树的模型时，不需要对数据进行归一化处理。不过，在使用线性模型（如逻辑回归）时<strong>不容忽视</strong>。支持向量机或神经网络，也希望数据是标准化的。</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<p>循化&#x2F;周期变量 周一~周日 </p>
<p>二元 只有两个类别的分类变量</p>
<blockquote>
<p>处理 NaN 值的另一种方法是将其作为一个全新的类别。这是处理 NaN 值最常用的方法。</p>
<p>所有未见过的新类别都将被映射为 “RARE”，而所有缺失值都将被映射为 “NONE”。</p>
</blockquote>
<p>例1：Kaggle 分类特征编码挑战赛中的 <em>cat-in-the-dat</em></p>
<ol>
<li><p>从目标变量来看，这是一个二元分类问题，可以先看一下目标变量的分布，观察到它是<strong>偏斜</strong>的。:yellow_heart: : :red_circle: &#x3D; 5:1</p>
</li>
<li><p>ord_2 特征包括6个不同的类别： - 冰冻 - 温暖 - 寒冷 - 较热 - 热 - 非常热</p>
</li>
</ol>
<p>简单的映射为数字，使用 <code>.map(静态字典)</code>替换数字：</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">映射字典</span></span><br><span class="line">mapping = &#123;</span><br><span class="line">&quot;Freezing&quot;: 0, </span><br><span class="line">&quot;Warm&quot;: 1, </span><br><span class="line">&quot;Cold&quot;: 2,</span><br><span class="line">&quot;Boiling Hot&quot;: 3, </span><br><span class="line">&quot;Hot&quot;: 4,</span><br><span class="line">&quot;Lava Hot&quot;: 5 </span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>

<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line">df = pd.read_csv(<span class="string">&quot;../input/cat_train.csv&quot;</span>) </span><br><span class="line"><span class="comment"># 取ord_2列，并使用映射将类别文本转换为数字</span></span><br><span class="line">df.loc[:, <span class="string">&quot;ord_2&quot;</span>] = df.ord_2.<span class="built_in">map</span>(mapping)</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>注：df.列名.map() df.列名.value_counts() df.列名.func() 很新的用法</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">df.ord_2.value_counts() <span class="comment"># 映射之前的数值计数</span></span><br><span class="line">Freezing       <span class="number">142726</span> </span><br><span class="line">Warm           <span class="number">124239</span></span><br><span class="line">Cold           <span class="number">97822</span></span><br><span class="line">Boiling Hot    <span class="number">84790</span></span><br><span class="line">Hot            <span class="number">67508</span></span><br><span class="line">Lava Hot       <span class="number">64840</span> </span><br><span class="line">Name: ord_2, dtype: int64</span><br><span class="line"><span class="comment"># 映射之之后的数值计数</span></span><br><span class="line"><span class="number">0.0</span>   <span class="number">142726</span></span><br><span class="line"><span class="number">1.0</span>   <span class="number">124239</span></span><br><span class="line"><span class="number">2.0</span>    <span class="number">97822</span></span><br><span class="line"><span class="number">3.0</span>    <span class="number">84790</span></span><br><span class="line"><span class="number">4.0</span>    <span class="number">67508</span></span><br><span class="line"><span class="number">5.0</span>    <span class="number">64840</span></span><br><span class="line">Name: ord_2, dtype: int64</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>注： 以上方法为 标签编码（Label Encoding）我们将每个类别编码为一个数字标签。sklearn 的 LabelEncoder提供了实现。</p>
</blockquote>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing </span><br><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line">df = pd.read_csv(<span class="string">&quot;../input/cat_train.csv&quot;</span>) </span><br><span class="line"><span class="comment"># 将缺失值填充为&quot;NONE&quot; </span></span><br><span class="line">df.loc[:, <span class="string">&quot;ord_2&quot;</span>] = df.ord_2.fillna(<span class="string">&quot;NONE&quot;</span>) </span><br><span class="line"><span class="comment"># LabelEncoder编码</span></span><br><span class="line">lbl_enc = preprocessing.LabelEncoder()</span><br><span class="line"><span class="comment"># 转换数据</span></span><br><span class="line">df.loc[:, <span class="string">&quot;ord_2&quot;</span>] = lbl_enc.fit_transform(df.ord_2.values)</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>注：</p>
<ul>
<li><p>LabelEncoder 并不能处理NaN值，所以需要先使用fillna填充。许多基于树的模型可直接使用 <strong>标签编码</strong>： - 决策树 - 随机森林 - 提升树 - 或任何一种提升树模型 - XGBoost - GBM - LightGBM。</p>
</li>
<li><p><strong>但是</strong> <strong>标签编码</strong> 不能用于线性模型、支持向量机或神经网络，因为它们希望数据是标准化的。</p>
<blockquote>
<p>Linear Models、SVM、Neural Networks 等都是运用数字大小，距离等连续性、有序性信息来学习参数的。如果输入的是标签编码【<strong>离散</strong>】，可能导致学习过程出现偏差，因为它们会<strong>默认</strong>标签编码代表了某种有序或距离关系。为了避免这种现象，通常使用“独热编码”（One-Hot Encoding）或者“哑变量”（Dummy Variables）等方法。one-hot下，每个类别都<strong>同等重要</strong>，不偏向任何方向，这样的编码更适合这类模型。这样当特征可选集合很大，会导致维度过高可用<strong>嵌入编码</strong>来降维。</p>
</blockquote>
</li>
</ul>
</blockquote>
<p>对线性模型采用<strong>二值化</strong>处理【比one-hot的更短，信息密集；但又比数字标签稀疏】：</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Freezing    --&gt; <span class="number">0</span> --&gt; <span class="number">0</span> <span class="number">0</span> <span class="number">0</span></span><br><span class="line">Warm        --&gt; <span class="number">1</span> --&gt; <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> </span><br><span class="line">Cold        --&gt; <span class="number">2</span> --&gt; <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> </span><br><span class="line">Boiling Hot --&gt; <span class="number">3</span> --&gt; <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> </span><br><span class="line">Hot         --&gt; <span class="number">4</span> --&gt; <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> </span><br><span class="line">Lava Hot    --&gt; <span class="number">5</span> --&gt; <span class="number">1</span> <span class="number">0</span> <span class="number">1</span></span><br></pre></td></tr></table></div></figure>

<p>这样单个标签变成了多个标签（每一维是一个特征），对于二值化数据我们可以采取稀疏性存储，简单说就是只存储有<code>1</code>的位置坐标。</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> sparse</span><br><span class="line"></span><br><span class="line">example = np.array(</span><br><span class="line">    [</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>], </span><br><span class="line">        [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>], </span><br><span class="line">        [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">    ] </span><br><span class="line">)</span><br><span class="line">sparse_example = sparse.csr_matrix(example)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(sparse_example.data.nbytes) <span class="comment"># 数据字节数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 全部的字节数 包含指针和索引</span></span><br><span class="line"><span class="built_in">print</span>(</span><br><span class="line">    sparse_example.data.nbytes +</span><br><span class="line">    sparse_example.indptr.nbytes +</span><br><span class="line">    sparse_example.indices.nbytes </span><br><span class="line">)</span><br></pre></td></tr></table></div></figure>

<p>将独热编码矩阵转换为密集矩阵，空间的节省更明显。</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成符合均匀分布的随机整数，维度为[1000000, 10000000]</span></span><br><span class="line">example = np.random.randint(<span class="number">1000</span>, size=<span class="number">1000000</span>)</span><br><span class="line"><span class="comment"># 独热编码，非稀疏矩阵</span></span><br><span class="line">ohe = preprocessing.OneHotEncoder(sparse=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 将随机数组展平</span></span><br><span class="line">ohe_example = ohe.fit_transform(example.reshape(-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Size of dense array: <span class="subst">&#123;ohe_example.nbytes&#125;</span>&quot;</span>) </span><br><span class="line"><span class="comment"># 独热编码，稀疏矩阵</span></span><br><span class="line">ohe = preprocessing.OneHotEncoder(sparse=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 将随机数组展平</span></span><br><span class="line">ohe_example = ohe.fit_transform(example.reshape(-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Size of sparse array: <span class="subst">&#123;ohe_example.data.nbytes&#125;</span>&quot;</span>) </span><br><span class="line">full_size = (</span><br><span class="line">    ohe_example.data.nbytes +</span><br><span class="line">    ohe_example.indptr.nbytes + </span><br><span class="line">    ohe_example.indices.nbytes </span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Full size of sparse array: <span class="subst">&#123;full_size&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>注：使用独热向量的默认前提是 你准备放弃标签编码的方式 不再关心特征变量之间的 <strong>顺序</strong> 关系</p>
<p>如果你想保留数据里面的顺序关系，可以尝试以下几种方法：</p>
<ol>
<li><p><strong>序数编码（Ordinal Encoding）</strong>：顺序编码是将类别赋予一个整型，这样可以保存类别之间的顺序。所以比如 “冰冻”(0) &lt; “温暖”(1) &lt; “寒冷”(2) &lt; “较热”(3) &lt; “热”(4) &lt; “非常热”(5)。</p>
<p>但是，这种编码虽然维持了顺序关系，却引入了一个问题，那就是相邻类别之间的距离变得有意义了。在上述的例子中，“冰冻”和“温暖”之间的“距离”和“温暖”和“寒冷”之间的“距离”是一样的，但实际情况可能并不这样。</p>
</li>
<li><p><strong>分段编码（Bucket Encoding）</strong>：你可以把它们划分为几个区间，比如0-0.2代表“冰冻”，0.2-0.4代表“寒冷”，以此类推。这样可以在一定程度上反映出顺序关系，但仍然可能存在相邻类别间的距离有意义的问题。</p>
</li>
<li><p><strong>Embedding</strong>：高维稀疏特征可以通过嵌入（embedding）在低维稠密空间中表示。对于有序类别分类，嵌入可以学习出它们的顺序关系。比如在NLP领域中，词嵌入（word embeddings）就是一种有效的表示方式，它不仅能反映词之间的相似度，也可以反映出一些词序关系。</p>
</li>
</ol>
<p>针对你的问题，如果“冰冻”，“温暖”，“寒冷”，“较热”，“热”，“非常热”之间的差异是线性的，可以选择序数编码。如果差异复杂，建议选择嵌入方法，通过训练来学习他们之间的相似性和顺序关系。</p>
<p>总之，看最后的效果，保有哪些信息最结果最有利，实际上是难以兼顾原始数据的 【模糊的】顺序性和量级关系。</p>
</blockquote>

        <h4 id="将特征变量转换为数值变量"   >
          <a href="#将特征变量转换为数值变量" class="heading-link"><i class="fas fa-link"></i></a><a href="#将特征变量转换为数值变量" class="headerlink" title="将特征变量转换为数值变量"></a>将特征变量转换为数值变量</h4>
      <p>在数据中，<em>ord_2</em> 的值为“热“的 id 有多少？【言外之意，用当前特征变量的定义域的某个取值在整体数据中出现的<strong>频率</strong>来代替它】</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [X]: df[df.ord_2 == <span class="string">&quot;Boiling Hot&quot;</span>].shape</span><br><span class="line">Out[X]: (<span class="number">84790</span>, <span class="number">25</span>) <span class="comment"># 说明Boiling Hot的频度是84790</span></span><br></pre></td></tr></table></div></figure>

<p>查看单个特征 的所有ID的频度</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">In [X]: df.groupby([<span class="string">&quot;ord_2&quot;</span>])[<span class="string">&quot;id&quot;</span>].count()</span><br><span class="line">Out[X]: </span><br><span class="line">ord_2</span><br><span class="line">Boiling Hot <span class="number">84790</span></span><br><span class="line">Cold 		<span class="number">97822</span> </span><br><span class="line">Freezing 	<span class="number">142726</span> </span><br><span class="line">Hot 		<span class="number">67508</span> 	</span><br><span class="line">Lava Hot 	<span class="number">64840</span> </span><br><span class="line">Warm 		<span class="number">124239</span> </span><br><span class="line">Name: <span class="built_in">id</span>, dtype: int64</span><br></pre></td></tr></table></div></figure>

<p>直接使用频度来替换 ID</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">In [X]: df.groupby([<span class="string">&quot;ord_2&quot;</span>])[<span class="string">&quot;id&quot;</span>].transform(<span class="string">&quot;count&quot;</span>)</span><br><span class="line">Out[X]:</span><br><span class="line"><span class="number">0</span>         <span class="number">67508.0</span></span><br><span class="line"><span class="number">1</span>        <span class="number">124239.0</span></span><br><span class="line"><span class="number">2</span>        <span class="number">142726.0</span> <span class="comment"># Freezing</span></span><br><span class="line"><span class="number">3</span>         <span class="number">64840.0</span></span><br><span class="line"><span class="number">4</span>         <span class="number">97822.0</span></span><br><span class="line">...</span><br><span class="line"><span class="number">599995</span>   <span class="number">142726.0</span> <span class="comment"># Freezing</span></span><br><span class="line"><span class="number">599996</span>    <span class="number">84790.0</span></span><br><span class="line"><span class="number">599997</span>   <span class="number">142726.0</span> <span class="comment"># Freezing</span></span><br><span class="line"><span class="number">599998</span>   <span class="number">124239.0</span></span><br><span class="line"><span class="number">599999</span>    <span class="number">84790.0</span></span><br><span class="line">Name: <span class="built_in">id</span>, Length: <span class="number">600000</span>, dtype: float64</span><br><span class="line"></span><br></pre></td></tr></table></div></figure>

<p>groupby类似于数据库的 联合查找，并增加了一个新的列 <strong>count</strong> 用于记录不同组合的频度</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">In [X]: df.groupby(</span><br><span class="line">   ...:     [</span><br><span class="line">   ...:        <span class="string">&quot;ord_1&quot;</span>,</span><br><span class="line">   ...:        <span class="string">&quot;ord_2&quot;</span></span><br><span class="line">   ...:     ]</span><br><span class="line">   ...: )[<span class="string">&quot;id&quot;</span>].count().reset_index(name=<span class="string">&quot;count&quot;</span>) </span><br><span class="line">Out[X]:</span><br><span class="line">ord_1        ord_2  count</span><br><span class="line"><span class="number">0</span>  Contributor  Boiling Hot <span class="number">15634</span></span><br><span class="line"><span class="number">1</span>  Contributor         Cold <span class="number">17734</span></span><br><span class="line"><span class="number">2</span>  Contributor     Freezing <span class="number">26082</span></span><br><span class="line"><span class="number">3</span>  Contributor          Hot <span class="number">12428</span></span><br><span class="line"><span class="number">4</span>  Contributor     Lava Hot <span class="number">11919</span></span><br><span class="line"><span class="number">5</span>  Contributor         Warm <span class="number">22774</span></span><br><span class="line"><span class="number">6</span>       Expert  Boiling Hot <span class="number">19477</span></span><br><span class="line"><span class="number">7</span>       Expert         Cold <span class="number">22956</span></span><br><span class="line"><span class="number">8</span>       Expert     Freezing <span class="number">33249</span></span><br><span class="line"><span class="number">9</span>       Expert          Hot <span class="number">15792</span></span><br><span class="line"><span class="number">10</span>      Expert     Lava Hot <span class="number">15078</span></span><br><span class="line"><span class="number">11</span>      Expert         Warm <span class="number">28900</span></span><br><span class="line"><span class="number">12</span> Grandmaster  Boiling Hot <span class="number">13623</span></span><br><span class="line"><span class="number">13</span> Grandmaster         Cold <span class="number">15464</span></span><br><span class="line"><span class="number">14</span> Grandmaster     Freezing <span class="number">22818</span></span><br><span class="line"><span class="number">15</span> Grandmaster          Hot <span class="number">10805</span></span><br><span class="line"><span class="number">16</span> Grandmaster     Lava Hot <span class="number">10363</span></span><br><span class="line"><span class="number">17</span> Grandmaster         Warm <span class="number">19899</span></span><br><span class="line"><span class="number">18</span>      Master  Boiling Hot <span class="number">10800</span></span><br><span class="line">...</span><br></pre></td></tr></table></div></figure>

<p>您现在一定已经注意到，我使用 id 列进行计数。不过，你也可以通过<strong>对列的组合</strong>进行分组，对其他列进行计数。</p>
<p>还有一个小窍门，就是从这些分类变量中创建新特征。</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">In [X]: df[<span class="string">&quot;new_feature&quot;</span>] = (</span><br><span class="line">   ...:     df.ord_1.astype(<span class="built_in">str</span>)</span><br><span class="line">   ...:     + <span class="string">&quot;_&quot;</span></span><br><span class="line">   ...:     + df.ord_2.astype(<span class="built_in">str</span>)</span><br><span class="line">   ...: )</span><br><span class="line">In [X]: df.new_feature </span><br><span class="line">Out[X]:</span><br><span class="line"><span class="number">0</span>                Contributor_Hot</span><br><span class="line"><span class="number">1</span>               Grandmaster_Warm</span><br><span class="line"><span class="number">2</span>                   nan_Freezing</span><br><span class="line"><span class="number">3</span>                Novice_Lava Hot</span><br><span class="line"><span class="number">4</span>               Grandmaster_Cold</span><br><span class="line">               ...</span><br><span class="line"><span class="number">599999</span>   Contributor_Boiling Hot</span><br><span class="line">Name: new_feature, Length: <span class="number">600000</span>, dtype: <span class="built_in">object</span></span><br></pre></td></tr></table></div></figure>

<p>那么，我们应该把哪些类别结合起来呢？这并没有一个简单的答案。这取决于您的数据和特征类型。一些<strong>领域知识</strong>对于创建这样的特征可能很有用。但是，如果你不担心内存和 CPU 的使用，你可以采用一种贪婪的方法，即创建许多这样的组合，然后使用一个模型来决定哪些特征是有用的，并保留它们。</p>
<blockquote>
<p>注：暴力的尝试在允许的条件下，尝试所有有效的组合 <code>n!</code> 是很稳妥的。</p>
</blockquote>
<p>简单步骤：</p>
<ul>
<li><p>填充 NaN 值（这一点非常重要！）</p>
</li>
<li><p>使用 scikit-learn 的 LabelEncoder 或映射字典进行标签编码，将它们转换为整数。</p>
</li>
<li><p>创建独热编码。是的，你可以跳过二值化！ </p>
</li>
<li><p>建模！我指的是机器学习。</p>
</li>
</ul>

        <h4 id="处理NaN值"   >
          <a href="#处理NaN值" class="heading-link"><i class="fas fa-link"></i></a><a href="#处理NaN值" class="headerlink" title="处理NaN值"></a>处理NaN值</h4>
      <ul>
<li>直接丢弃这一行的所有数据 不建议</li>
<li>作为 全新类別 使用频度值来代替</li>
<li>填充一个固定值 <strong>均值&#x2F;中位数&#x2F;众数&#x2F;零</strong> 可能导致数据偏斜</li>
<li>使用模型进行预测填充： 可以用一个回归或者分类模型根据其它特征预测 NaN 值</li>
<li>使用插值填充：插值法尤其适用于<strong>时间序列</strong>数据。插值法是在已知的值之间进行估计，例如线性插值或者多项式插值</li>
</ul>
<p><strong>罕见类别</strong>，即只在样本总数中占很小比例的类别。当模型或项目上线时，您在 <em>ord_2</em> 列中得到了一个在训练中不存在的类别。在这种情况下，模型管道会抛出一个错误，您对此无能为力。如果出现这种情况，那么可能是生产中的管道出了问题。如果这是预料之中的，那么您就必须修改您的模型管道，并在这六个类别中加入一个新类别。</p>
<p>这个新类别被称为 “罕见 “类别。罕见类别是一种不常见的类别，可以包括许多不同的类别。您也可以尝试使用<strong>近邻模型</strong>来 “预测 “未知类别。请记住，如果您预测了这个类别，它就会成为训练数据中的一个类别。</p>
<blockquote>
<p>注：简单说，在实际使用的过程中提取得到的特征结果，不在自己的处理范围之内，如PDF的版本号不是数字而是乱码。</p>
</blockquote>
<p>一种可行的弥补方式：把测试集合中数据 不训练但是<strong>提取特征的定义域</strong>时带上他们，避免测试集合中出现大量未考虑到的特征变量的值。</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing </span><br><span class="line"><span class="comment"># 读取训练集</span></span><br><span class="line">train = pd.read_csv(<span class="string">&quot;../input/cat_train.csv&quot;</span>) </span><br><span class="line"><span class="comment"># 读取测试集</span></span><br><span class="line">test = pd.read_csv(<span class="string">&quot;../input/cat_test.csv&quot;</span>) </span><br><span class="line"><span class="comment"># 将测试集&quot;target&quot;列全部置为-1 因为不需要考虑</span></span><br><span class="line">test.loc[:, <span class="string">&quot;target&quot;</span>] = -<span class="number">1</span></span><br><span class="line"><span class="comment"># 将训练集、测试集沿行拼接</span></span><br><span class="line">data = pd.concat([train, test]).reset_index(drop=<span class="literal">True</span>) </span><br><span class="line"><span class="comment"># 将除&quot;id&quot;和&quot;target&quot;列的其他特征列名取出</span></span><br><span class="line">features = [x <span class="keyword">for</span> x <span class="keyword">in</span> train.columns <span class="keyword">if</span> x <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&quot;id&quot;</span>, <span class="string">&quot;target&quot;</span>]] </span><br><span class="line"><span class="comment"># 遍历特征</span></span><br><span class="line"><span class="keyword">for</span> feat <span class="keyword">in</span> features:</span><br><span class="line">    <span class="comment"># 标签编码</span></span><br><span class="line">    lbl_enc = preprocessing.LabelEncoder()</span><br><span class="line">    <span class="comment"># 将空值替换为&quot;NONE&quot;,并将该列格式变为str</span></span><br><span class="line">    temp_col = data[feat].fillna(<span class="string">&quot;NONE&quot;</span>).astype(<span class="built_in">str</span>).values </span><br><span class="line">    <span class="comment"># 转换数值</span></span><br><span class="line">    data.loc[:, feat] = lbl_enc.fit_transform(temp_col)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据&quot;target&quot;列将训练集与测试集分开</span></span><br><span class="line">train = data[data.target != -<span class="number">1</span>].reset_index(drop=<span class="literal">True</span>) </span><br><span class="line">test = data[data.target == -<span class="number">1</span>].reset_index(drop=<span class="literal">True</span>)</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>注：以上方法只有在静态环境中有效，就是测试集合和训练集都在手里。而对于实时系统，测试集是动态到来的，拿不到也不知道可能的异常值。这样我们需要建立<strong>未知类别</strong>。</p>
</blockquote>
<p>这与自然语言处理问题非常相似。我们总是基于固定的词汇建立模型。增加词汇量就会增加模型的大小。像 BERT 这样的转换器模型是在 ~30000 个单词（英语）的基础上训练的。因此，当有新词输入时，我们会将其标记为 UNK（未知）。</p>
<p>因此，您可以假设测试数据与训练数据具有相同的类别，也可以在训练数据中引入<strong>罕见或未知</strong>类别，以处理测试数据中的新类别。</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In [X]: df.ord_4 = df.ord_4.fillna(<span class="string">&quot;NONE&quot;</span>)</span><br><span class="line">In [X]: df.loc[</span><br><span class="line">   ...:     df[<span class="string">&quot;ord_4&quot;</span>].value_counts()[df[<span class="string">&quot;ord_4&quot;</span>]].values &lt; <span class="number">2000</span>, <span class="comment"># 实际数值小于两千</span></span><br><span class="line">   ...:    <span class="string">&quot;ord_4&quot;</span></span><br><span class="line">   ...: ] = <span class="string">&quot;RARE&quot;</span></span><br><span class="line">In [X]: df.ord_4.value_counts()</span><br></pre></td></tr></table></div></figure>



<p>剩下的，暂时没看懂：</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">encoded_dfs = []</span><br><span class="line">    <span class="keyword">for</span> fold <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        df_train = df[df.kfold != fold].reset_index(drop=<span class="literal">True</span>) </span><br><span class="line">        df_valid = df[df.kfold == fold].reset_index(drop=<span class="literal">True</span>) </span><br><span class="line">        <span class="keyword">for</span> column <span class="keyword">in</span> features:</span><br><span class="line">            <span class="comment"># 目标编码</span></span><br><span class="line">            mapping_dict = <span class="built_in">dict</span>(df_train.groupby(column)[<span class="string">&quot;income&quot;</span>].mean() )</span><br><span class="line">            df_valid.loc[:, column + <span class="string">&quot;_enc&quot;</span>] = df_valid[column].<span class="built_in">map</span>(mapping_dict)</span><br><span class="line">        encoded_dfs.append(df_valid)</span><br><span class="line">    encoded_df = pd.concat(encoded_dfs, axis=<span class="number">0</span>) </span><br></pre></td></tr></table></div></figure>






        <h3 id="7-特征工程"   >
          <a href="#7-特征工程" class="heading-link"><i class="fas fa-link"></i></a><a href="#7-特征工程" class="headerlink" title="7 特征工程"></a>7 特征工程</h3>
      <ol>
<li><p>特征工程不仅仅是从数据中创建新特征，还包括不同类型的归一化和转换</p>
</li>
<li><p>填补缺失值 可以用 K近邻来填补 【训练回归模型预测】</p>
<blockquote>
<p>对于数值数据来说，比填充 0 更有效的方法之一是使用平均值进行填充。您也可以尝试使用该特征所有值的中位数来填充，或者使用最常见的值来填充缺失值。这样做的方法有很多。</p>
</blockquote>
</li>
<li><p>您还可以为包含或不包含分类数据的数值数据创建许多其他特征。生成许多特征的一个简单方法就是创建一堆多项式特征,a^2</p>
</li>
</ol>

        <h3 id="8-特征选择"   >
          <a href="#8-特征选择" class="heading-link"><i class="fas fa-link"></i></a><a href="#8-特征选择" class="headerlink" title="8 特征选择"></a>8 特征选择</h3>
      <ol>
<li>删除方差小的，几乎不变就像常量，没有区分度</li>
<li>删除相关性高的特征，两个表现力一样，只保留一个</li>
<li>单变量特征选择 【值得一学】</li>
<li>特征剔除法 【贪心法】</li>
<li>特征重要性 【部分树模型支持】</li>
</ol>

        <h3 id="9-图像分割"   >
          <a href="#9-图像分割" class="heading-link"><i class="fas fa-link"></i></a><a href="#9-图像分割" class="headerlink" title="9 图像分割"></a>9 图像分割</h3>
      <p>需要实践。</p>

        <h3 id="10-文本分类或回归方法"   >
          <a href="#10-文本分类或回归方法" class="heading-link"><i class="fas fa-link"></i></a><a href="#10-文本分类或回归方法" class="headerlink" title="10 文本分类或回归方法"></a>10 文本分类或回归方法</h3>
      <p>没有新东西。</p>

        <h3 id="11-超参数优化"   >
          <a href="#11-超参数优化" class="heading-link"><i class="fas fa-link"></i></a><a href="#11-超参数优化" class="headerlink" title="11 超参数优化"></a>11 超参数优化</h3>
      <p>需要上手。</p>

        <h3 id="12-模型堆叠"   >
          <a href="#12-模型堆叠" class="heading-link"><i class="fas fa-link"></i></a><a href="#12-模型堆叠" class="headerlink" title="12 模型堆叠"></a>12 模型堆叠</h3>
      <ol>
<li>简单地说，组合相关性不高的模型比组合相关性很高的模型效果更好。</li>
<li>使用<strong>AUC作为概率排序</strong> 【强！】</li>
</ol>

        <h3 id="13-可重复代码与模型"   >
          <a href="#13-可重复代码与模型" class="heading-link"><i class="fas fa-link"></i></a><a href="#13-可重复代码与模型" class="headerlink" title="13 可重复代码与模型"></a>13 可重复代码与模型</h3>
      <p>Docker</p>
</div><footer class="post-footer"><div class="post-ending ending"><div class="ending__text">------ 本文结束，感谢您的阅读 ------</div></div><div class="post-copyright copyright"><div class="copyright-author"><span class="copyright-author__name">本文作者: </span><span class="copyright-author__value"><a href="https://anqing2120.github.io">一诚</a></span></div><div class="copyright-link"><span class="copyright-link__name">本文链接: </span><span class="copyright-link__value"><a href="https://anqing2120.github.io/2023/12/26/ML-learning/">https://anqing2120.github.io/2023/12/26/ML-learning/</a></span></div><div class="copyright-notice"><span class="copyright-notice__name">版权声明: </span><span class="copyright-notice__value">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" rel="external nofollow" target="_blank">BY-NC-SA</a> 许可协议。转载请注明出处！</span></div></div><div class="post-tags"><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="https://anqing2120.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span></div><nav class="post-paginator paginator"><div class="paginator-next"><a class="paginator-next__link" href="/2023/09/26/%E7%99%BB%E5%B1%B1%E4%B9%8B%E7%BF%92/"><span class="paginator-prev__text">登山之習</span><span class="paginator-next__icon"><i class="fas fa-angle-right"></i></span></a></div></nav></footer></div></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><div class="sidebar-nav"><span class="sidebar-nav-toc current">文章目录</span><span class="sidebar-nav-ov">站点概览</span></div><section class="sidebar-toc"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Approaching-Almost-Any-Machine-Learning-Problem"><span class="toc-number">1.</span> <span class="toc-text">
          Approaching (Almost) Any Machine Learning Problem</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E7%8E%AF%E5%A2%83"><span class="toc-number">1.1.</span> <span class="toc-text">
          1 环境</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%92%8C-%E6%9C%89%E7%9B%91%E7%9D%A3"><span class="toc-number">1.2.</span> <span class="toc-text">
          2 无监督 和 有监督</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E4%BA%A4%E5%8F%89%E6%A3%80%E9%AA%8C"><span class="toc-number">1.3.</span> <span class="toc-text">
          3 交叉检验</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87"><span class="toc-number">1.4.</span> <span class="toc-text">
          4 评价指标</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BB%93%E6%9E%84%E7%BB%84%E7%BB%87"><span class="toc-number">1.5.</span> <span class="toc-text">
          5 模型的结构组织</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-%E5%A4%84%E7%90%86%E5%88%86%E7%B1%BB%E5%8F%98%E9%87%8F"><span class="toc-number">1.6.</span> <span class="toc-text">
          6 处理分类变量</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B0%86%E7%89%B9%E5%BE%81%E5%8F%98%E9%87%8F%E8%BD%AC%E6%8D%A2%E4%B8%BA%E6%95%B0%E5%80%BC%E5%8F%98%E9%87%8F"><span class="toc-number">1.6.1.</span> <span class="toc-text">
          将特征变量转换为数值变量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%84%E7%90%86NaN%E5%80%BC"><span class="toc-number">1.6.2.</span> <span class="toc-text">
          处理NaN值</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B"><span class="toc-number">1.7.</span> <span class="toc-text">
          7 特征工程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9"><span class="toc-number">1.8.</span> <span class="toc-text">
          8 特征选择</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2"><span class="toc-number">1.9.</span> <span class="toc-text">
          9 图像分割</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%88%96%E5%9B%9E%E5%BD%92%E6%96%B9%E6%B3%95"><span class="toc-number">1.10.</span> <span class="toc-text">
          10 文本分类或回归方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-%E8%B6%85%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96"><span class="toc-number">1.11.</span> <span class="toc-text">
          11 超参数优化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-%E6%A8%A1%E5%9E%8B%E5%A0%86%E5%8F%A0"><span class="toc-number">1.12.</span> <span class="toc-text">
          12 模型堆叠</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-%E5%8F%AF%E9%87%8D%E5%A4%8D%E4%BB%A3%E7%A0%81%E4%B8%8E%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.13.</span> <span class="toc-text">
          13 可重复代码与模型</span></a></li></ol></li></ol></section><!-- ov = overview--><section class="sidebar-ov hide"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/images/lianq.png" alt="avatar"></div><p class="sidebar-ov-author__text">刚乾日新</p></div><div class="sidebar-ov-state"><a class="sidebar-ov-state-item sidebar-ov-state-item--posts" href="/archives/"><div class="sidebar-ov-state-item__count">3</div><div class="sidebar-ov-state-item__name">归档</div></a></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="知识共享许可协议" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg"></a></div></section><div class="sidebar-reading"><div class="sidebar-reading-info"><span class="sidebar-reading-info__text">你已阅读了 </span><span class="sidebar-reading-info__num">0</span><span class="sidebar-reading-info__perc">%</span></div><div class="sidebar-reading-line"></div></div></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright © 2023</span><span class="footer__icon"><i class="fas fa-heart"></i></span><span>一诚</span></div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script src="/js/utils.js?v=2.8.0"></script><script src="/js/stun-boot.js?v=2.8.0"></script><script src="/js/scroll.js?v=2.8.0"></script><script src="/js/header.js?v=2.8.0"></script><script src="/js/sidebar.js?v=2.8.0"></script></body></html>